{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline with Apache Beam.\n",
    "\n",
    "In this notebook I want to create a pipeline for training, using Apache Beam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pyts.image as pyti\n",
    "import pyts.approximation as pyta\n",
    "import re\n",
    "from typing import Dict\n",
    "import apache_beam as beam\n",
    "\n",
    "LABELS=['Break_in', 'Steady_state', 'Severe', 'Failure']\n",
    "\n",
    "\n",
    "def retrieve_label(filename: str):\n",
    "    file_string = os.path.basename(filename)\n",
    "    (_, _, cut_event, _) = re.split(r\"_|\\.\", file_string)\n",
    "    cut_event = int(cut_event)\n",
    "    if cut_event < 50:\n",
    "        label = LABELS[0]\n",
    "    elif cut_event < 175:\n",
    "        label = LABELS[1]\n",
    "    elif cut_event < 250:\n",
    "        label = LABELS[2]\n",
    "    else:\n",
    "        label = LABELS[3]\n",
    "    return label\n",
    "\n",
    "def rescale(x, low=0, high=1):\n",
    "    return (high - low) * (x - np.min(x)) / (np.max(x) - np.min(x)) + low\n",
    "\n",
    "\n",
    "def get_gasf(cutter_sample): \n",
    "    cutter_acos = rescale(cutter_sample, low=-1, high=1)\n",
    "    cutter_acos = np.arccos(cutter_acos)\n",
    "    cutter_acos = np.asarray(cutter_acos).transpose()\n",
    "    cutter_acos = cutter_acos.reshape(1, -1)\n",
    "    paa = pyta.PiecewiseAggregateApproximation(window_size=4)\n",
    "    fcutter_paa = paa.transform(cutter_acos) # Piecewise Aggregate Approximation applied to the time-domain data\n",
    "    nelem = fcutter_paa.shape[1]\n",
    "    gasf = pyti.GramianAngularField(image_size=nelem, method='summation') # Grammar Angular Summation Field\n",
    "    fcutter_gasf = gasf.transform(fcutter_paa)\n",
    "    image = rescale(fcutter_gasf, low=0, high=1)\n",
    "    return image\n",
    "\n",
    "class ProduceGasf(beam.DoFn):\n",
    "    \"\"\"Produces GASF images from a CSV given file.\"\"\"\n",
    "    def __init__(self, nparts: Dict[str, int]):\n",
    "        beam.DoFn.__init__(self)\n",
    "        self._numrows = 2048\n",
    "        self._nparts = nparts\n",
    "    \n",
    "    def process(self, filename: str):\n",
    "        label = retrieve_label(filename)\n",
    "        with open(filename, newline='\\n') as f:\n",
    "            contents = csv.reader(f)\n",
    "            contents = list(contents)\n",
    "            num_rows = len(contents)\n",
    "            for _ in range(self._nparts[label]):\n",
    "                idx = np.random.choice(num_rows - self._numrows, 1)[0] # Random choice of the index\n",
    "                selected_contents = contents[idx:(idx+self._numrows)]\n",
    "                selected_contents = np.array([[float(x[0]), float(x[1]), float(x[2])] for x in selected_contents]) # Take only the required columns.\n",
    "                image = get_gasf(selected_contents)\n",
    "                yield {\n",
    "                    'label': label,\n",
    "                    'image': image            \n",
    "                }\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Break_in', 2)\n",
      "('Steady_state', 1)\n",
      "('Severe', 4)\n",
      "('Failure', 3)\n"
     ]
    }
   ],
   "source": [
    "filenames = [\n",
    "            \"c_1_037.csv\", # \"c_1_003.csv\", # Break_in\n",
    "            \"c_1_151.csv\", # \"c_1_062.csv\", # Steady state\n",
    "            \"c_1_205.csv\", # \"c_1_240.csv\", # Severe\n",
    "            \"c_1_294.csv\", # \"c_1_305.csv\" # Failure\n",
    "        ]\n",
    "filenames = [ f\"../../Datasets/PHM2010Challenge/c1/c1/{fn}\" for fn in filenames]\n",
    "sampling = {\"Break_in\": 4, \"Steady_state\": 2, \"Severe\": 4, \"Failure\": 4} # Specify occurrences for each class for undersampling.\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | \"Create file patterns\" >> beam.Create(filenames)\n",
    "        | \"Transform data into GASF images\" >> beam.ParDo(ProduceGasf(sampling))\n",
    "        | 'Get the classes' >> beam.Map(lambda x: x['label'])\n",
    "        | 'Count elements per key' >> beam.combiners.Count.PerElement()\n",
    "        | beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count how many files of each class there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Steady_state', 125)\n",
      "('Failure', 66)\n",
      "('Severe', 75)\n",
      "('Break_in', 49)\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | \"Create file patterns\"  >> beam.io.fileio.MatchFiles(\"../../Datasets/PHM2010Challenge/c1/c1/*.csv\")\n",
    "        | \"Read matches\" >> beam.io.fileio.ReadMatches()\n",
    "        | \"Get labels\"  >> beam.Map(lambda f: retrieve_label(f.metadata.path))\n",
    "        | \"Count labels\" >> beam.combiners.Count.PerElement()\n",
    "        | beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize into TFRecords\n",
    "\n",
    "Serialize the images and labels into TFRecords. We will use undersampling of the `Steady-state` class.\n",
    "\n",
    "#### TODO: Use PuLP or scipy.optimize to find integers for the partitions to balance the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "labels_to_int = {\"Break_in\": 0, \"Steady_state\": 1, \"Severe\": 2, \"Failure\": 3}\n",
    "\n",
    "# Serialize the label and image\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"\n",
    "    Returns a bytes_list from a string / byte.\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Create an int64 feature from a label\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(image, label):\n",
    "    \"\"\"Serialized the image and its corresponding label\"\"\"\n",
    "    feature = {\n",
    "        \"image\": _bytes_feature(image.tobytes()),\n",
    "        \"label\": _int64_feature(labels_to_int[label])\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "\n",
    "class WriteToTFRecords(beam.DoFn):\n",
    "\n",
    "    def process(self, element, OUTPUT_FOLDER):\n",
    "        \"\"\"Serialize the element into a TFRecord.\n",
    "\n",
    "        The element is a dictionary {'label': label, 'image': np.array}\n",
    "        \"\"\"\n",
    "        image = element.get(\"image\").astype(dtype=np.float32)\n",
    "        label = element.get(\"label\")\n",
    "        path = os.path.join(OUTPUT_FOLDER, label + str(hash(random.random()))  + \".TFRecord\")\n",
    "\n",
    "        with tf.io.TFRecordWriter(path) as writer:\n",
    "            example = serialize_example(image, label)\n",
    "            writer.write(example)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    }
   ],
   "source": [
    "sampling = {\"Break_in\": 5, \"Steady_state\": 2, \"Severe\": 4, \"Failure\": 4} # Specify occurrences for each class for undersampling.\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | \"Create file patterns\"  >> beam.io.fileio.MatchFiles(\"../../Datasets/PHM2010Challenge/c1/c1/*.csv\")\n",
    "        | \"Read matches\"  >> beam.io.fileio.ReadMatches()\n",
    "        | \"Get filenames\" >> beam.Map(lambda x: x.metadata.path)\n",
    "        | \"Transform data into GASF images\" >> beam.ParDo(ProduceGasf(sampling))\n",
    "        | \"Write into TFRecords\" >> beam.ParDo(WriteToTFRecords(), OUTPUT_FOLDER=\"GASF_Images\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the classes are somehow balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Break_in', 245)\n",
      "('Failure', 264)\n",
      "('Steady_state', 250)\n",
      "('Severe', 300)\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | \"Create file patterns\" >> beam.io.fileio.MatchFiles(\"GASF_Images/*.TFRecord\")\n",
    "        | \"Read matches\" >> beam.io.fileio.ReadMatches()\n",
    "        | \"Get filenames\" >> beam.Map(lambda x: os.path.basename(x.metadata.path) )\n",
    "        | \"Split names into classes and the rest\" >> beam.Regex.split(r'([A-Za-z]+(_[A-Za-z]+)?)([0-9]+\\.TFRecord)')\n",
    "        | \"Extract classes\" >> beam.Map(lambda x: x[0])\n",
    "        | \"Count classes\" >> beam.combiners.Count.PerElement()\n",
    "        | beam.FlatMap(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems ok\n",
    "\n",
    "\n",
    "# Read files from TFRecord and train the model."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
